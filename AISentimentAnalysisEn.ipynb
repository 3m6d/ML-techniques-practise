{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/3m6d/ML-techniques-practise/blob/main/AISentimentAnalysisEn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "-aaeABDq6-8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBtfjagBkih6"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "df = pd.read_json('/content/drive/MyDrive/Beauty.jsonl', lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvDbO0Y-iLe5"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXWQUvIjLxdZ"
      },
      "source": [
        "Renaming the Rating column to Sentiment column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sc3akjVKcJl"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Remove any 'neutral' ratings equal to 3\n",
        "df = df[df['rating'] != 3]\n",
        "\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Encode 4s and 5s as 1 (positive sentiment) and 1s and 2s as 0 (negative sentiment)\n",
        "df['Sentiment'] = np.where(df['rating'] > 3, 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55PyWKimMJcN"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"The Number of Reviews less than rating 3\")\n",
        "df[df['rating'] < 3].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8J0xW9WaMdGU"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"The Number of Reviews greater than 3\")\n",
        "df[df['rating'] > 3].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-07ZObdMkN2"
      },
      "outputs": [],
      "source": [
        "print(\"The Size of Dataset\",df.shape)\n",
        "print('Distribution of Positive and Negative Reviews, Three being the threshold')\n",
        "df.hist('rating')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVZOqTGbMyWq"
      },
      "outputs": [],
      "source": [
        "\n",
        "X = df['text']\n",
        "y = df['Sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nm0C3h_lU0yj"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "\n",
        "\n",
        "def cleanText(raw_text, remove_stopwords=True, stemming=False, split_text=False, \\\n",
        "             ):\n",
        "    '''\n",
        "    Convert a raw review to a cleaned review\n",
        "    '''\n",
        "    text = BeautifulSoup(raw_text, 'lxml').get_text()  #remove html\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)  # remove non-character\n",
        "    words = letters_only.lower().split() # convert to lower case\n",
        "\n",
        "    if remove_stopwords: # remove stopword\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        stops.remove('not')\n",
        "        stops.remove('no')\n",
        "        words = [w for w in words if not w in stops]\n",
        "    if stemming==True: # stemming\n",
        "        stemmer = PorterStemmer()\n",
        "        stemmer = SnowballStemmer('english')\n",
        "        words = [stemmer.stem(w) for w in words]\n",
        "\n",
        "    if split_text==True:  # split text\n",
        "        return (words)\n",
        "\n",
        "    return( \" \".join(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soDz6eeWNR1s"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "X_cleaned = []\n",
        "\n",
        "for d in X:\n",
        "    X_cleaned.append(cleanText(d))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZklCE-5sPJRj"
      },
      "outputs": [],
      "source": [
        "countVect = CountVectorizer(min_df = 50, ngram_range = (1,2),strip_accents='unicode', binary=True)\n",
        "X_all_countVect = countVect.fit_transform(X_cleaned)\n",
        "\n",
        "print(\"Number of features : %d \\n\" %len(countVect.get_feature_names_out())) #1722\n",
        "print(\"Show some feature names : \\n\", countVect.get_feature_names_out()[::1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTthl1m7Qjw-"
      },
      "outputs": [],
      "source": [
        "# Split data into training and testing sets\n",
        "X_train_countVect, X_test_countVect, y_train, y_test = train_test_split(\n",
        "    X_all_countVect, y, test_size=0.2, random_state=42, stratify=y\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9-krM6pSMbP"
      },
      "outputs": [],
      "source": [
        "def knn_classifier(X_train_countVect,y_train,X_test_countVect,y_test,target):\n",
        "    classifier=KNeighborsClassifier(n_neighbors=5)\n",
        "    classifier.fit(X_train_countVect,y_train)\n",
        "\n",
        "    y_pred=classifier.predict(X_test_countVect)\n",
        "\n",
        "    y_pred_train = classifier.predict(X_train_countVect)\n",
        "    print('KNN Results:')\n",
        "    print(\"KNN Accuracy:\",metrics.accuracy_score(y_test,y_pred))\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"Confusion Matrix\",confusion_matrix(y_test, y_pred))\n",
        "    print(\"KNN Train Accuracy:\",metrics.accuracy_score(y_train,y_pred_train))\n",
        "    print(classification_report(y_train, y_pred_train))\n",
        "\n",
        "    return metrics.accuracy_score(y_test,y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgwAQHkSSkA7"
      },
      "outputs": [],
      "source": [
        "def svc_classifier(X_train_countVect,y_train,X_test_countVect,y_test,target_names):\n",
        "  from sklearn import svm\n",
        "  clf=svm.SVC(kernel='linear')\n",
        "  clf.fit(X_train_countVect,y_train)\n",
        "\n",
        "  y_pred=clf.predict(X_test_countVect)\n",
        "\n",
        "  y_pred_train =clf.predict(X_train_countVect)\n",
        "\n",
        "  print('SVM Results:')\n",
        "  print(\"SVM Accuracy:\",metrics.accuracy_score(y_test,y_pred))\n",
        "  print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "  print(\"Confusion Matrix\",confusion_matrix(y_test, y_pred))\n",
        "  print(\"SVM Train Accuracy:\",metrics.accuracy_score(y_train,y_pred_train))\n",
        "  print(classification_report(y_train, y_pred_train, target_names=target_names))\n",
        "\n",
        "  return metrics.accuracy_score(y_test,y_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gj7yN4rgS44-"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Naive Bayes classifier\n",
        "def nb_classifier(X_train_countVect,y_train,X_test_countVect,y_test,target_names):\n",
        "\n",
        "  clf = MultinomialNB()\n",
        "  clf.fit(X_train_countVect.toarray(),y_train)\n",
        "\n",
        "  y_pred=clf.predict(X_test_countVect)\n",
        "\n",
        "  y_pred_train =clf.predict(X_train_countVect)\n",
        "  print('NB Results:')\n",
        "  print(\"MNB Accuracy:\",metrics.accuracy_score(y_test,y_pred))\n",
        "  print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "  print(\"Confusion Matrix\",confusion_matrix(y_test, y_pred))\n",
        "  print(\"MNB Train Accuracy:\",metrics.accuracy_score(y_train,y_pred_train))\n",
        "  print(classification_report(y_train, y_pred_train, target_names=target_names))\n",
        "\n",
        "  return metrics.accuracy_score(y_test,y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSisak5dTIaK"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Logistic Regression\n",
        "def lr_classifier(X_train_countVect,y_train,X_test_countVect,y_test,target_names):\n",
        "  lr = LogisticRegression()\n",
        "  lr.fit(X_train_countVect.toarray(), y_train)\n",
        "\n",
        "\n",
        "  y_pred=lr.predict(X_test_countVect)\n",
        "\n",
        "  y_pred_train =lr.predict(X_train_countVect)\n",
        "  print('LR Results:')\n",
        "  print(\"LR Accuracy:\",metrics.accuracy_score(y_test,y_pred))\n",
        "  print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "  print(\"Confusion Matrix\",confusion_matrix(y_test, y_pred))\n",
        "  print(\"LR Train Accuracy:\",metrics.accuracy_score(y_train,y_pred_train))\n",
        "  print(classification_report(y_train, y_pred_train, target_names=target_names))\n",
        "\n",
        "  return metrics.accuracy_score(y_test,y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfA-URG_QUm1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUmGIZ14RTxv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "\n",
        "knn = []\n",
        "svm = []\n",
        "dt =[]\n",
        "nb =[]\n",
        "lr =[]\n",
        "\n",
        "rus = RandomUnderSampler(random_state=777)\n",
        "X_RUS, y_RUS = rus.fit_resample(X_all_countVect, y)\n",
        "target_names = ['Positive','Negative']\n",
        "skf = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
        "for train_index, test_index in skf.split(X_RUS, y_RUS):\n",
        "  X_train_countVect = X_RUS[train_index]\n",
        "  y_train = y_RUS[train_index]\n",
        "  X_test_countVect = X_RUS[test_index]\n",
        "  y_test = y_RUS[test_index]\n",
        "\n",
        "\n",
        "  knn_mean = knn_classifier(X_train_countVect,y_train,X_test_countVect,y_test,target_names)\n",
        "  knn.append(knn_mean)\n",
        "  nb_mean = nb_classifier(X_train_countVect,y_train,X_test_countVect,y_test,target_names)\n",
        "  nb.append(nb_mean)\n",
        "  lr_mean = lr_classifier(X_train_countVect,y_train,X_test_countVect,y_test,target_names)\n",
        "  lr.append(lr_mean)\n",
        "  svm_mean = svc_classifier(X_train_countVect,y_train,X_test_countVect,y_test,target_names)\n",
        "  svm.append(svm_mean)\n",
        "\n",
        "\n",
        "  print('The Accuracy for KNN:',sum(knn)/len(knn))\n",
        "  print('The Accuracy for SVM:',sum(svm)/len(svm))\n",
        "  print('The Accuracy for DT:',sum(dt)/len(dt))\n",
        "  print('The Accuracy for MNB:',sum(nb)/len(nb))\n",
        "  print('The Accuracy for LR:',sum(lr)/len(lr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1nhS2OVh_tM"
      },
      "outputs": [],
      "source": [
        "def preprocess_and_vectorize(X_train, X_test, vectorizer):\n",
        "    X_train_vec = vectorizer.fit_transform(X_train)\n",
        "    X_test_vec = vectorizer.transform(X_test)\n",
        "    return X_train_vec, X_test_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nD1dZZrGls4Q"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dCP6nfRm1ZD"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82ueE5Gxm3dQ"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate model performance\n",
        "def evaluate_model(y_true, y_pred, y_probs=None):\n",
        "    metrics = {\n",
        "        'Precision': precision_score(y_true, y_pred),\n",
        "        'Recall': recall_score(y_true, y_pred),\n",
        "        'F1-Score': f1_score(y_true, y_pred),\n",
        "    }\n",
        "    if y_probs is not None:\n",
        "        metrics['ROC-AUC'] = roc_auc_score(y_true, y_probs)\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTHCTXbLm9bq"
      },
      "outputs": [],
      "source": [
        "# 1. Naive Bayes Classifier\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_tfidf, y_train)\n",
        "nb_preds = nb_model.predict(X_test_tfidf)\n",
        "nb_probs = nb_model.predict_proba(X_test_tfidf)[:, 1]\n",
        "print(\"Naive Bayes Metrics:\", evaluate_model(y_test, nb_preds, nb_probs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yhA4m_vnA3q"
      },
      "outputs": [],
      "source": [
        "# 2. Support Vector Machines (SVM)\n",
        "svm_model = SVC(kernel='linear', probability=True, random_state=42)\n",
        "svm_model.fit(X_train_tfidf, y_train)\n",
        "svm_preds = svm_model.predict(X_test_tfidf)\n",
        "svm_probs = svm_model.predict_proba(X_test_tfidf)[:, 1]\n",
        "print(\"SVM Metrics:\", evaluate_model(y_test, svm_preds, svm_probs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6s2Y-_-DnD7t"
      },
      "outputs": [],
      "source": [
        "# 3. Logistic Regression\n",
        "log_reg_model = LogisticRegression(random_state=42)\n",
        "log_reg_model.fit(X_train_tfidf, y_train)\n",
        "log_reg_preds = log_reg_model.predict(X_test_tfidf)\n",
        "log_reg_probs = log_reg_model.predict_proba(X_test_tfidf)[:, 1]\n",
        "print(\"Logistic Regression Metrics:\", evaluate_model(y_test, log_reg_preds, log_reg_probs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oldkZSWgnJCy"
      },
      "outputs": [],
      "source": [
        "# Importing Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Function to preprocess and vectorize data\n",
        "def preprocess_and_vectorize(X_train, X_test, vectorizer):\n",
        "    X_train_vec = vectorizer.fit_transform(X_train)\n",
        "    X_test_vec = vectorizer.transform(X_test)\n",
        "    return X_train_vec, X_test_vec\n",
        "\n",
        "# Function to get Word2Vec features\n",
        "def get_w2v_features(tokens, model, vector_size):\n",
        "    features = []\n",
        "    for sentence in tokens:\n",
        "        vec = np.zeros(vector_size)\n",
        "        count = 0\n",
        "        for word in sentence:\n",
        "            if word in model.wv.key_to_index:\n",
        "                vec += model.wv[word]\n",
        "                count += 1\n",
        "        if count > 0:\n",
        "            vec /= count\n",
        "        features.append(vec)\n",
        "    return np.array(features)\n",
        "\n",
        "# Function to evaluate model\n",
        "def evaluate_model(model, X_test_vec, y_test_enc):\n",
        "    y_pred = model.predict(X_test_vec)\n",
        "    accuracy = accuracy_score(y_test_enc, y_pred)\n",
        "    print(f\"Accuracy: {accuracy:.2f}\\n\")\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test_enc, y_pred))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_enc, y_pred))\n",
        "    print(\"-\" * 50)\n",
        "    return accuracy\n",
        "\n",
        "# Step 1: Create Dummy Data\n",
        "data = {\n",
        "    \"text\": [\n",
        "        \"I love this product, it's amazing!\",\n",
        "        \"This is the worst experience I've ever had.\",\n",
        "        \"Absolutely fantastic! Highly recommend.\",\n",
        "        \"Not great, would not buy again.\",\n",
        "        \"The quality is superb, really satisfied.\",\n",
        "        \"Terrible, broke after one use.\",\n",
        "        \"Decent product for the price.\",\n",
        "        \"Awful customer service, very disappointed.\",\n",
        "        \"Excellent value for money, very happy.\",\n",
        "        \"It's okay, nothing special but works fine.\"\n",
        "    ],\n",
        "    \"sentiment\": [\"positive\", \"negative\", \"positive\", \"negative\", \"positive\",\n",
        "                  \"negative\", \"neutral\", \"negative\", \"positive\", \"neutral\"]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Step 2: Preprocessing\n",
        "X = df['text']\n",
        "y = df['sentiment']\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Encoding labels for consistency\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_enc = label_encoder.fit_transform(y_train)\n",
        "y_test_enc = label_encoder.transform(y_test)\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=500, ngram_range=(1, 2))  # Using unigrams and bigrams\n",
        "X_train_tfidf, X_test_tfidf = preprocess_and_vectorize(X_train, X_test, tfidf_vectorizer)\n",
        "\n",
        "# Bag of Words (BoW) Vectorization\n",
        "bow_vectorizer = CountVectorizer(max_features=500, ngram_range=(1, 2))\n",
        "X_train_bow, X_test_bow = preprocess_and_vectorize(X_train, X_test, bow_vectorizer)\n",
        "\n",
        "# Word2Vec Vectorization\n",
        "X_train_tokens = [sentence.split() for sentence in X_train]\n",
        "X_test_tokens = [sentence.split() for sentence in X_test]\n",
        "\n",
        "w2v_model = Word2Vec(sentences=X_train_tokens, vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
        "X_train_w2v = get_w2v_features(X_train_tokens, w2v_model, vector_size=100)\n",
        "X_test_w2v = get_w2v_features(X_test_tokens, w2v_model, vector_size=100)\n",
        "\n",
        "# Step 3: Model Training and Evaluation\n",
        "methods = {\n",
        "    \"TF-IDF\": (X_train_tfidf, X_test_tfidf),\n",
        "    \"Bag of Words\": (X_train_bow, X_test_bow),\n",
        "    \"Word2Vec\": (X_train_w2v, X_test_w2v)\n",
        "}\n",
        "\n",
        "models = {}\n",
        "results = {}\n",
        "\n",
        "for method, (X_train_vec, X_test_vec) in methods.items():\n",
        "    print(f\"Evaluating with {method}...\")\n",
        "    model = LogisticRegression(max_iter=200)\n",
        "    model.fit(X_train_vec, y_train_enc)\n",
        "    accuracy = evaluate_model(model, X_test_vec, y_test_enc)\n",
        "    results[method] = accuracy\n",
        "    models[method] = model\n",
        "\n",
        "# Compare Results\n",
        "print(\"\\nComparison of Accuracy:\")\n",
        "for method, accuracy in results.items():\n",
        "    print(f\"{method}: {accuracy:.2f}\")\n",
        "\n",
        "# Step 4: Predict Sentiment for User Input\n",
        "print(\"\\nEnter sentences to predict their sentiment:\")\n",
        "while True:\n",
        "    sentence = input(\"Enter a sentence (or type 'exit' to quit): \")\n",
        "    if sentence.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    sentence_vecs = {\n",
        "        \"TF-IDF\": tfidf_vectorizer.transform([sentence]),\n",
        "        \"Bag of Words\": bow_vectorizer.transform([sentence]),\n",
        "        \"Word2Vec\": get_w2v_features([sentence.split()], w2v_model, vector_size=100)\n",
        "    }\n",
        "\n",
        "    print(\"Predictions:\")\n",
        "    for method, vec in sentence_vecs.items():\n",
        "        pred_label = models[method].predict(vec)\n",
        "        sentiment = label_encoder.inverse_transform(pred_label)\n",
        "        print(f\"{method}: {sentiment[0]}\")\n",
        "    print(\"-\" * 50)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMLk7Qf5uCUJSJGTmwcY2OF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}